## Prerequisites

Have the necessary environment setup as needed as individual project 4. This project is an extension of Project 4. Additional packages to be installed are pandas, numpy, tqdm, openpyxl and scipy. All of them can be installed by a simple `pip install <package_name>`


## Important Files
1. automate.py: This file triggers the `pacman.py` python process with the necessary variables set. The global variables listed in `automate.py` can be used to tweak the resulting `pacman` process run. Details of the variables are documented in the `automate.py` file. The current values of the variables are optimized for running locally.

2. combine.py: This file is used to load the results (i.e excel files) generated by `automate.py` and combine them to a summary sheet. Comments are provided in `combine.py` regarding it's usage.

## Running the code locally

1. Run `python3 automate.py`
Wait for the processes to complete, if the default parameters are used then a folder named `local_results` is created and the excel files (one for SARSA and one for Approximate Q Agent) are placed in it.

2. Run `python3 combine.py`
If the default parameters are used then a new excel file is created and has a sheet named summary in it. The summary sheet has eight columns and `TRAINING_COUNT + TESTING_COUNT` number of rows. These values are present in the `automate.py` file.

* s_score_avg: Reward per episode, averaged over `ITER` runs. `ITER` is a variable in the `automate.py` file, it denotes the number of runs to make for each type of agent. While generating results for the report, we have set it's value to 60. However that takes a lot of time to run, so we set the default value to 1.

* s_score_avg_cumu: Contains cumulative values of the `s_score_avg` column.

* s_vict_avg: For all the training episodes this column has the value 0 (since there is no victory or defeat during training). In the testing episodes, this contains the value of the averaged value of the agent won at a particular episode number.

* s_vict_avg_cumu: Contains cumulative values of the `s_vict_avg` column.

The above columns are for the SARSA agent, similarly four other columns for the q learning agent exist with the same schema.

* q_score_avg
* q_score_avg_cumu
* q_vict_avg
* q_vict_avg_cumu

Plotting `s_score_avg` against `q_score_avg` gives us the average reward of both the agents per epsiode plotted over episodes. These are the plots which are present in the submitted report.


## NOTES
If run locally using the default parameters the plots would be different than what is in the report. This is because the default value of `ITER` (no of runs per agent) is 1. For the report we have generated values with `ITER` set to 60.
All required documents for submission are in the `submission docs` folder

## Contributions
Contribution summary has been reported in the Team Effectiveness Report. GIT was used in the project, so git log is avaialble.
